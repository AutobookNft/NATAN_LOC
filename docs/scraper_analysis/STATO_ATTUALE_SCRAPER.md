# üìä ANALISI STATO ATTUALE - SCRAPER ALBO PRETORIO

**Data analisi**: 13 novembre 2025  
**Obiettivo**: Sistema multi-tenant per scraping albi pretori italiani

---

## üîç STATO ATTUALE - COSA ABBIAMO

### **Scraper Esistenti**

#### **1. `scrape_albo_firenze_v2.py`** - HTML Scraping
- ‚úÖ **Funzionante**: Scraping pagine HTML albo pretorio Firenze
- ‚úÖ **Target**: `https://accessoconcertificato.comune.fi.it/AOL/Affissione/ComuneFi/Page`
- ‚úÖ **Estrazione**: 
  - Tipo atto, direzione
  - Numero registro, numero atto
  - Date inizio/fine pubblicazione
  - Oggetto atto
  - Link PDF allegati
- ‚úÖ **Features**:
  - Paginazione automatica (estrae totale pagine)
  - Rate limiting (sleep tra richieste)
  - Download PDF opzionale
  - Salvataggio JSON
- ‚úÖ **Librerie**: `requests`, `BeautifulSoup4`
- ‚ö†Ô∏è **Limitazioni**: 
  - Solo HTML parsing (no JS rendering)
  - Specifico per struttura DOM Firenze
  - No multi-tenant support
  - No MongoDB integration

#### **2. `scrape_firenze_deliberazioni.py`** - API Scraping
- ‚úÖ **Funzionante**: Scraping via API REST Firenze
- ‚úÖ **Target**: `https://accessoconcertificato.comune.fi.it/trasparenza-atti-cat/searchAtti`
- ‚úÖ **Estrazione**:
  - Deliberazioni Giunta (DG)
  - Deliberazioni Consiglio (DC)
  - Determinazioni Dirigenziali (DD)
  - Decreti Sindacali (DS)
  - Ordinanze Dirigenziali (OD)
- ‚úÖ **Features**:
  - **MongoDB integration** completa (tenant-aware)
  - Estrazione testo da PDF
  - Chunking intelligente
  - Generazione embeddings
  - Cost tracking (OpenAI tokens)
  - Download PDF parallelo
  - Salvataggio JSON backup
  - Statistiche dettagliate
- ‚úÖ **Librerie**: `requests`, `asyncio`
- ‚úÖ **Integration**: `PAActMongoDBImporter` per import MongoDB
- üéØ **Ottimo esempio** per sistema target!

#### **3. `pa_act_mongodb_importer.py`** - MongoDB Importer
- ‚úÖ **Componente riutilizzabile** per import atti PA
- ‚úÖ **Features**:
  - Estrazione testo PDF (PyPDF2, pdfplumber)
  - Chunking intelligente (~2000 chars, 200 overlap)
  - Embeddings generation (OpenAI/Ollama)
  - Multi-tenant support (tenant_id)
  - Cost tracking dettagliato
  - Dry-run mode per test
  - Statistics tracking
- ‚úÖ **Storage MongoDB**: Collection `documents` con:
  - `document_type: "pa_act"`
  - `tenant_id` per isolamento
  - Metadata completi (protocol, date, source)
  - Chunks con embeddings per semantic search

---

## üíé PUNTI DI FORZA ATTUALI

### **Architettura MongoDB Multi-Tenant**
‚úÖ Sistema gi√† pensato per multi-tenancy  
‚úÖ Collection `documents` con `tenant_id`  
‚úÖ Importer generico e riutilizzabile  
‚úÖ Supporto embeddings per semantic search  
‚úÖ Cost tracking integrato

### **Pattern Code Quality**
‚úÖ Codice pulito e ben strutturato  
‚úÖ Error handling robusto  
‚úÖ Logging dettagliato  
‚úÖ Async support (asyncio)  
‚úÖ CLI arguments ben definiti

### **Features Avanzate**
‚úÖ Estrazione PDF multi-library (PyPDF2, pdfplumber)  
‚úÖ Chunking intelligente per retrieval  
‚úÖ Rate limiting per scraping etico  
‚úÖ Progress tracking JSON per frontend  
‚úÖ Backup JSON + MongoDB storage

---

## ‚ö†Ô∏è LIMITAZIONI ATTUALI

### **Specificit√† Firenze**
‚ùå Scraper hardcoded per Firenze (URL, DOM structure, API endpoints)  
‚ùå No generalizzazione per altri comuni  
‚ùå No auto-detection piattaforma  
‚ùå No configurazione per-tenant

### **Scalabilit√†**
‚ùå No queue system per scraping distribuito  
‚ùå No scheduling automatico  
‚ùå No retry logic robusto  
‚ùå No monitoring/dashboard

### **Pattern Recognition**
‚ùå No ML per identificare strutture simili  
‚ùå No fallback automatico se struttura cambia  
‚ùå No confidence scoring  
‚ùå Parsing regex fragile

### **Management**
‚ùå No admin panel per gestione tenant/comuni  
‚ùå No API REST per accesso dati  
‚ùå No sistema di notifiche (scraping fallito, etc.)  
‚ùå No health checks

---

## üéØ LIBRERIE GI√Ä IN USO

### **Core Scraping**
```python
requests       # HTTP client
BeautifulSoup4 # HTML parsing
asyncio        # Async operations
```

### **PDF Processing**
```python
PyPDF2         # PDF text extraction
pdfplumber     # Alternative PDF extraction
```

### **Database**
```python
pymongo        # MongoDB driver (via MongoDBService)
```

### **AI/Embeddings**
```python
openai         # Embeddings generation (via AIRouter)
# + support Ollama (locale)
```

---

## üìã ANALISI STRUTTURA FIRENZE

### **Piattaforma Identificata**
- **Vendor**: Sistema custom Comune di Firenze
- **CMS**: Non standard (custom web app)
- **Tecnologie**:
  - Frontend: HTML + JavaScript (pagine dinamiche)
  - Backend: API REST JSON
  - PDF storage: File system web-exposed

### **URL Patterns**
```
Albo Pretorio HTML:
https://accessoconcertificato.comune.fi.it/AOL/Affissione/ComuneFi/Page?page=N

API Atti:
https://accessoconcertificato.comune.fi.it/trasparenza-atti-cat/searchAtti
(POST con JSON payload)
```

### **DOM Structure (HTML Scraping)**
```html
<!-- Ogni atto in card div -->
<div class="card concorso-card multi-line">
  <!-- Tipo atto, direzione -->
  <!-- N¬∞ registro: XX/YYYY -->
  <!-- N¬∞ atto: XX/YYYY -->
  <!-- Inizio pubblicazione: DD/MM/YYYY -->
  <!-- Fine pubblicazione: DD/MM/YYYY -->
  <!-- Oggetto: testo lungo... -->
  <!-- Link PDF: <a href="...pdf"> -->
</div>
```

### **API Structure**
```json
POST /trasparenza-atti-cat/searchAtti
{
  "oggetto": "",
  "numeroAdozione": "",
  "competenza": "DG",
  "annoAdozione": "2024",
  "tipiAtto": ["DG"]
}

Response: Array di atti con:
- numeroAdozione
- tipoAtto
- oggetto
- dataAdozione
- competenza
- annoAdozione
- allegati: [{id, link, contentType}]
```

---

## üöÄ PROSSIMI STEP

### **FASE 1: ANALISI COMUNI (IN CORSO)**
1. ‚úÖ Firenze analizzato
2. üîÑ Analizzare 9+ comuni toscani:
   - Prato, Livorno, Arezzo, Siena
   - Pistoia, Lucca, Grosseto
   - Massa, Pisa
   - + Comuni piccoli/medi per variet√†
3. Identificare pattern comuni e piattaforme ricorrenti
4. Documentare differenze e similarit√†

### **FASE 2: RICERCA DOCUMENTAZIONE**
- Best practices web scraping etico
- Librerie avanzate (Scrapy, Selenium, Playwright)
- ML per pattern recognition
- Anti-detection strategies
- Rate limiting best practices

### **FASE 3: DESIGN ARCHITETTURA**
- Base Scraper class (abstract)
- Platform-specific scrapers (Halley, Insiel, etc.)
- Auto-detection engine
- Configuration system per-tenant
- Queue system (Celery + RabbitMQ)
- Monitoring dashboard

---

## üí° NOTE TECNICHE

### **Cosa Funziona Bene**
- MongoDB importer √® production-ready
- Cost tracking accurato e utile
- Chunking strategy efficace (2000 chars, 200 overlap)
- Async architecture scalabile
- Error handling robusto

### **Cosa Migliorare**
- Generalizzare parsing (attualmente troppo specifico)
- Aggiungere retry logic con exponential backoff
- Implementare circuit breaker per API instabili
- Caching per evitare ri-scraping stesso atto
- Deduplication basata su hash contenuto

### **Cosa Aggiungere**
- ML classifier per auto-detect piattaforma
- Fallback scrapers per handling variazioni DOM
- Confidence scoring per validare dati estratti
- Admin dashboard (Streamlit/Gradio)
- API REST per accesso programmatico
- Notification system (email/Slack quando scraping fallisce)

---

**Status**: ‚úÖ Analisi scraper esistente COMPLETATA  
**Next**: üîÑ Analisi manuale 10+ comuni toscani IN CORSO
